import cv2
import mediapipe as mp
import socket
import math
import numpy as np  # arrays, matrices, and solvePnP helpers

# --- CAMERA / IMAGE SETTINGS ---
# Rough field-of-view estimates for a 640x480 webcam.
FOV_X = 90   # horizontal FOV in degrees
FOV_Y = 45   # vertical FOV in degrees (estimate)
DISPLAY_PIXEL_HEIGHT = 480
DISPLAY_PIXEL_WIDTH = 640


# --- UTILITY FUNCTIONS ---
def focal_length(fov, dimension):
    # Convert FOV in radians to focal length in pixels.
    return dimension / (2 * math.tan(fov / 2))

# These helpers are mostly kept for reference; PnP gives us 3D pose directly.
def pixel_to_angle(pixels, dimension, focal_length):
    return math.atan((pixels - dimension / 2) / focal_length)

def angle_to_position(angle_x, angle_y, distance):
    x = distance * math.tan(angle_x)
    y = distance * math.tan(angle_y)
    return x, y

def pixel_to_position(px, py, distance):
    fx = focal_length(math.radians(FOV_X), DISPLAY_PIXEL_WIDTH)
    fy = focal_length(math.radians(FOV_Y), DISPLAY_PIXEL_HEIGHT)
    angle_x = pixel_to_angle(px, DISPLAY_PIXEL_WIDTH, fx) + math.radians(CAMERA_ANGLE - DISPLAY_ANGLE)
    angle_y = pixel_to_angle(py, DISPLAY_PIXEL_HEIGHT, fy)
    # These X/Y estimates are less accurate than PnP results, kept here for context in case PnP breaks and
    # we need to revert to the older method
    return angle_to_position(angle_x, angle_y, distance)


# --- PnP MODEL AND CAMERA INTRINSICS ---

# 1) Minimal 3D face model in meters, with the origin at the nose tip.
#    These values were generated by ChatGPT based on average human face proportions, we'll probably have to 
#    tweak them for better accuracy.
model_points_3d = np.array([
    (0.0,   0.0,   0.0),   # Nose tip (LM 1) - model origin
    (0.0,  -0.110, 0.0),   # Chin (LM 152)
    (-0.032, -0.015, 0.010),  # Left eye inner corner (LM 133)
    (0.032,  -0.015, 0.010),  # Right eye inner corner (LM 362)
    (-0.050, -0.070, 0.000),  # Left mouth corner (LM 61)
    (0.050,  -0.070, 0.000)   # Right mouth corner (LM 291)
], dtype=np.float64)

# Corresponding indices in MediaPipe's 468-point face mesh
face_landmarks_indices = [1, 152, 133, 362, 61, 291]

# 2) Camera intrinsics (K) built from FOV and image size.
FX = focal_length(math.radians(FOV_X), DISPLAY_PIXEL_WIDTH)
FY = focal_length(math.radians(FOV_Y), DISPLAY_PIXEL_HEIGHT)
CX = DISPLAY_PIXEL_WIDTH / 2
CY = DISPLAY_PIXEL_HEIGHT / 2

camera_matrix = np.array([
    [FX, 0.0, CX],
    [0.0, FY, CY],
    [0.0, 0.0, 1.0]
], dtype=np.float64)

# Assume no lens distortion. Real webcams usually have some distortion apparently but this
# isn't really working anyways so let's ignore it for now.
dist_coeffs = np.zeros((4, 1))

# --- UDP OUTPUT ---
# UDP stuff for sending position updates to Unity. No need to bother with this.
print("Setting up UDP socket")
UDP_IP = "127.0.0.1"
UDP_PORT = 5005
sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

print("Starting MediaPipe Face Tracking")

# Initialize MediaPipe Face Mesh
mp_face_mesh = mp.solutions.face_mesh
# Lower this if detection is flaky, or add min_tracking_confidence for more stability.
# It controls how certain the model must be that a face is present to initialize tracking.
face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1, min_detection_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# Open default webcam
cap = cv2.VideoCapture(0)

# Exponential smoothing to reduce jitter, kinda makes it seem slower though so
# prediction would be a great addition.
alpha = 0.3
smoothed_x, smoothed_y, smoothed_z = 0, 0, 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Mirror the image so it feels natural
    frame = cv2.flip(frame, 1)
    h, w, _ = frame.shape
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Run Face Mesh
    results = face_mesh.process(rgb_frame)

    if results.multi_face_landmarks:
        face_landmarks = results.multi_face_landmarks[0]
        
        # 1) Collect 2D image points (convert from normalized [0..1] to pixels)
        image_points_2d = []
        for index in face_landmarks_indices:
            landmark = face_landmarks.landmark[index]
            x = landmark.x * w
            y = landmark.y * h
            image_points_2d.append((x, y))
        image_points_2d = np.array(image_points_2d, dtype=np.float64)

        # 2) Estimate pose (rvec, tvec) with PnP
        #    tvec is the model's translation in camera coordinates (meters).
        success, rvec, tvec = cv2.solvePnP(
            model_points_3d,
            image_points_2d,
            camera_matrix,
            dist_coeffs,
            flags=cv2.SOLVEPNP_EPNP  # Fast; try SOLVEPNP_ITERATIVE if you want extra robustness
        )

        if success:
            current_x = tvec[0, 0]
            current_y = tvec[1, 0]
            current_z = tvec[2, 0]

            # 3) Smooth the position (initialize on first valid frame)
            if smoothed_z == 0:
                smoothed_x, smoothed_y, smoothed_z = current_x, current_y, current_z
            else:
                smoothed_x = alpha * current_x + (1 - alpha) * smoothed_x
                smoothed_y = alpha * current_y + (1 - alpha) * smoothed_y
                smoothed_z = alpha * current_z + (1 - alpha) * smoothed_z

            # 4) Visualize: project the model origin (nose tip) back to 2D and draw a marker
            nose_tip_2d, _ = cv2.projectPoints(
                model_points_3d[0], rvec, tvec, camera_matrix, dist_coeffs
            )
            nx, ny = int(nose_tip_2d[0][0][0]), int(nose_tip_2d[0][0][1])
            cv2.circle(frame, (nx, ny), 10, (0, 0, 255), -1)

            # Log the smoothed 3D position
            print(f"PosX={smoothed_x:.2f}m, PosY={smoothed_y:.2f}m, PosZ={smoothed_z:.2f}m")

            # Optional: draw the full face mesh overlay
            mp_drawing.draw_landmarks(
                frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1)
            )

            # 5) Send UDP update. X/Y are negated to match the client's expected coordinate system.
            message = f"{0-smoothed_x:.3f},{0-smoothed_y:.3f},{smoothed_z:.3f}"
            sock.sendto(message.encode('utf-8'), (UDP_IP, UDP_PORT))

    # Show the preview window
    cv2.imshow("MediaPipe Face Tracking (PnP 3D)", frame)

    # Quit with 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Cleanup
cap.release()
cv2.destroyAllWindows()
sock.close()